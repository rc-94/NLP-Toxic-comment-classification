{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aEisj5YOc88C",
        "dH29huZuc_2V"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Level GNN**"
      ],
      "metadata": {
        "id": "QNJTmKbnc5Ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "aEisj5YOc88C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuQJ-Y2De8sC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl"
      ],
      "metadata": {
        "id": "NuV5R1AODF6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d10b8ebb-9ef2-4517-d3e2-bba2d221bf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl\n",
            "  Downloading dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dgl-cu101"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8K33yf36bpP",
        "outputId": "eb449bd9-9579-4fee-a89b-48bf412783c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dgl-cu101\n",
            "  Downloading dgl_cu101-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (36.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (1.21.5)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.6.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu101) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu101) (1.24.3)\n",
            "Installing collected packages: dgl-cu101\n",
            "Successfully installed dgl-cu101-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vffi7Ee-KUSX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb9ffb2a-f098-4caf-960d-fc8c1a2a07ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 32.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"DGLBACKEND\"] = \"pytorch\""
      ],
      "metadata": {
        "id": "gdLiB3c_DRAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # TF 2.1\n",
        "random.seed(SEED)\n",
        "#seed(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "vHDOC0T3fclR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from dgl import save_graphs, load_graphs\n",
        "from dgl.data import DGLDataset\n",
        "import dgl.nn.pytorch as dglnn\n",
        "from dgl.utils import expand_as_pair\n",
        "import dgl.function as fn\n",
        "import torch.nn as nn\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score  \n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "8_UBWYlpOOZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4aeb51-8c81-4bcc-d1e5-f0b68b5ee499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEeLcBdpUClE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7830add6-4798-48e9-9dfe-646716d2c180"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "dH29huZuc_2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subwords_to_merge(tokenized_sequence, sentence, verbose):\n",
        "  for i in range(len(tokenized_sequence)) :\n",
        "    while 'Ġ' in tokenized_sequence[i] :\n",
        "      tokenized_sequence[i] = tokenized_sequence[i].replace('Ġ','')\n",
        "  words = sentence.split(' ')\n",
        "  if verbose :\n",
        "    print(\"words\", words)\n",
        "    print(\"token\", tokenized_sequence)\n",
        "  i = 0\n",
        "  j = 0\n",
        "  cpt = 0\n",
        "  n_words = len(words)\n",
        "  list_subwords_to_merge = []\n",
        "\n",
        "  while cpt != n_words :\n",
        "    if tokenized_sequence[j] == words[i] :\n",
        "      cpt = cpt + 1\n",
        "      i = i + 1\n",
        "      j = j + 1\n",
        "    else :\n",
        "      tmp_word = tokenized_sequence[j]\n",
        "      tmp_merge = [j]\n",
        "      while tmp_word != words[i] :\n",
        "        j = j+1\n",
        "        tmp_word = tmp_word + tokenized_sequence[j]\n",
        "        tmp_merge.append(j)\n",
        "      list_subwords_to_merge.append(tmp_merge)\n",
        "      cpt = cpt + 1\n",
        "      i = i + 1\n",
        "      j = j + 1\n",
        "  return list_subwords_to_merge\n",
        "\n",
        "\n",
        "def get_embedding(comment, tokenizer, model, verbose):\n",
        "  tokenized_sequence = tokenizer.tokenize(comment)\n",
        "  subwords_indices = subwords_to_merge(tokenized_sequence, comment, verbose)\n",
        "  encoded_input = tokenizer(comment, return_tensors='pt')\n",
        "  output = model(**encoded_input)\n",
        "  outputseq = output.last_hidden_state[0]\n",
        "\n",
        "  final_embd = []\n",
        "  i = 0\n",
        "\n",
        "  while i != len(outputseq):\n",
        "    inside = False\n",
        "    for k in subwords_indices :\n",
        "      first, last = k[0], k[-1]\n",
        "      if i in range(first, last) :\n",
        "        inside = True\n",
        "        if (last+1)>len(outputseq):\n",
        "          merge, _ = torch.max(outputseq[first::], 0)\n",
        "        else:\n",
        "          merge, _ = torch.max(outputseq[first:(last+1)], 0)\n",
        "        final_embd.append(list(merge.detach().numpy()))\n",
        "        i =  i + len(k)\n",
        "        \n",
        "    if inside == False:\n",
        "      final_embd.append(list(outputseq[i].detach().numpy()))\n",
        "      i = i + 1\n",
        "\n",
        "  final_embd = torch.tensor(np.array(final_embd))\n",
        "  return final_embd"
      ],
      "metadata": {
        "id": "8wsFfT2jlcMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_process(sentence):\n",
        "  comment = sentence.replace('\\n', ' ')\n",
        "  while '\\\"' in comment :\n",
        "    comment = comment.replace('\\\"', '')\n",
        "  while \"\\'\" in comment :\n",
        "    comment = comment.replace(\"\\'\", '')\n",
        "  while ':' in comment :\n",
        "    comment = comment.replace(':', '')\n",
        "  while '.' in comment :\n",
        "    comment = comment.replace('.', '')\n",
        "  while '@' in comment :\n",
        "    comment = comment.replace('@', '')\n",
        "  while '+' in comment :\n",
        "    comment = comment.replace('+', '')\n",
        "  while '=' in comment:\n",
        "    comment = comment.replace('=', '')\n",
        "  while '&' in comment :\n",
        "    comment = comment.replace('&', '')\n",
        "  while ')' in comment or '(' in comment:\n",
        "    comment = comment.replace(')', '').replace('(', '')\n",
        "  while ',' in comment or ':' in comment or ';' in comment:\n",
        "    comment = comment.replace(\":\", '').replace(',', '').replace(';', '')\n",
        "  for c in comment :\n",
        "    if c.isascii() == False :\n",
        "      comment = comment.replace(c, '')\n",
        "  n1 = comment.count('!')\n",
        "  n2 = comment.count('?')\n",
        "  for i in range(n1):\n",
        "    comment = comment.replace('!', ' !')\n",
        "  for i in range(n2):\n",
        "    comment = comment.replace('?', ' ?')\n",
        "  while '  ' in comment :\n",
        "    comment = comment.replace('  ', ' ')\n",
        "  return comment\n",
        "  \n",
        "class ToxicCommentDataset(DGLDataset):\n",
        "    def __init__(self, save_dir):\n",
        "        super().__init__(name='toxic', save_dir = save_dir)\n",
        "        #self.save_dir = \n",
        "\n",
        "    def process(self):\n",
        "        tic = time.time()\n",
        "        p , q = 1, 2\n",
        "        self.graphs = []\n",
        "        self.labels = []\n",
        "\n",
        "        train_df = pd.read_csv('/content/drive/My Drive/IASD_tmp/NLP/train.csv')\n",
        "        tic = time.time()\n",
        "\n",
        "        #delete comments that belongs to several classes\n",
        "        idx_to_del = []\n",
        "        for row in train_df.itertuples():\n",
        "          if sum(row[3::])> 1 :\n",
        "            idx_to_del.append(row[0])\n",
        "          \n",
        "        train_df = train_df.drop(idx_to_del)\n",
        "\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "        model = GPT2Model.from_pretrained('gpt2')\n",
        "\n",
        "        cpt = 0\n",
        "        n_too_long = 0\n",
        "\n",
        "\n",
        "        toxic = train_df[train_df['toxic'] == 0]\n",
        "        obscene = toxic[toxic['obscene']==0]\n",
        "        threat = obscene[obscene['threat']==0]\n",
        "        insult = threat[threat['insult']==0]\n",
        "        neutral = insult[insult['identity_hate']==0] #df with neutral comment\n",
        "\n",
        "        toxic = train_df[train_df['toxic'] == 1]\n",
        "        obscene = train_df[train_df['obscene'] == 1]\n",
        "        threat = train_df[train_df['threat'] == 1]\n",
        "        insult = train_df[train_df['insult'] == 1]\n",
        "        identity_hate = train_df[train_df['identity_hate'] == 1]\n",
        "\n",
        "        final_df = pd.concat([toxic, obscene, threat, insult, identity_hate, neutral]) #merge all the df together in the right order\n",
        "        for row in final_df.itertuples():\n",
        "      \n",
        "          cpt = cpt + 1\n",
        "          print(\"cpt:\", cpt)\n",
        "          \n",
        "         \n",
        "          if cpt in [1344, 1345, 1445, 1446, 1528, 1529, 3243, 3244, 3773, 3776, 11888] :\n",
        "            continue\n",
        "          if len(self.graphs) == 10000 or len(self.labels) == 10000:\n",
        "            if len(self.graphs) == len(self.labels):\n",
        "              break\n",
        "            else : \n",
        "              print(\"Graphs and labels different shapes\")\n",
        "              return -1\n",
        "          \n",
        "          \n",
        "          comment = sentence_process(row[2])\n",
        "          words = comment.split(' ')\n",
        "         \n",
        "          while '' in words :\n",
        "            words.remove('')\n",
        "          \n",
        "          for w in words :\n",
        "            if w.isnumeric() and len(w)>=5 :\n",
        "              words.remove(w)\n",
        "          \n",
        "          words = list(filter(lambda s: 'http' not in s, words))\n",
        "            \n",
        "          comment = \" \".join(words)\n",
        "\n",
        "          if len(words)>100:\n",
        "            print(\"Too long\")\n",
        "            n_too_long += 1\n",
        "            continue\n",
        "\n",
        "\n",
        "          #print(\"comment:\", comment)\n",
        "          if row[3] == 0 and row[4] == 0 and row[5] ==0 and row[6] ==0 and row[7] == 0 and row[8] == 0:\n",
        "            self.labels.append(0)\n",
        "          elif row[3] == 1 :\n",
        "            self.labels.append(1)\n",
        "          elif row[4] == 1 :\n",
        "            self.labels.append(6)\n",
        "          elif row[5] == 1 :\n",
        "            self.labels.append(2)\n",
        "          elif row[6] == 1 :\n",
        "            self.labels.append(3)\n",
        "          elif row[7] == 1 :\n",
        "            self.labels.append(4)\n",
        "          else: \n",
        "            self.labels.append(5)\n",
        "      \n",
        "          #Get embeddings for the sentence\n",
        "          verbose = False\n",
        "          outputseq = get_embedding(comment, tokenizer, model, verbose)\n",
        "          \n",
        "          #Isolate words\n",
        "          words = comment.split(' ')\n",
        "\n",
        "          assert len(words) == len(outputseq)\n",
        "  \n",
        "          #Fill the embedding dic for the comment\n",
        "          dic_embeddings = {}\n",
        "          for k, word in enumerate(words) :\n",
        "            dic_embeddings[word] = outputseq[k].detach().numpy()\n",
        "          \n",
        "          dic_nodes_int = {}\n",
        "          doubles = {}\n",
        "          total_doubles = 0\n",
        "          list_embeddings = []\n",
        "\n",
        "          for word in words :\n",
        "            if word in dic_nodes_int : ### cas de doublons\n",
        "              dic_nodes_int[word] = [dic_nodes_int[word], len(dic_nodes_int)]\n",
        "              doubles[word] = 0\n",
        "              total_doubles = total_doubles + 1\n",
        "            else :\n",
        "              dic_nodes_int[word] = len(dic_nodes_int) + total_doubles\n",
        "\n",
        "            list_embeddings.append(dic_embeddings[word])\n",
        "\n",
        "          list_embeddings = np.array(list_embeddings)\n",
        "\n",
        "          #### BOTTOM ####\n",
        "          bottom_list_src = []\n",
        "          bottom_list_dst = []\n",
        "\n",
        "          for j, word in enumerate(words) : \n",
        "\n",
        "            neighbors = words[max((j-p),0):j] + words[j:(j+p+1)]\n",
        "            neighbors_idx = [m for m in range(max((j-p), 0), max((j+p+1), len(neighbors)))]\n",
        "            dic_nodes_int_neighbors = []\n",
        "\n",
        "            for r, neigh in enumerate(neighbors):\n",
        "              if neigh in doubles : \n",
        "                dic_nodes_int_neighbors.append(neighbors_idx[r])\n",
        "              else :\n",
        "                dic_nodes_int_neighbors.append(dic_nodes_int[neigh])\n",
        "\n",
        "            bottom_list_src = bottom_list_src + [j for k in range(len(neighbors))] + dic_nodes_int_neighbors # Add edges in both directions\n",
        "            bottom_list_dst = bottom_list_dst + dic_nodes_int_neighbors + [j for k in range(len(neighbors))] #\n",
        "\n",
        "          #### MIDDLE ####\n",
        "          middle_list_src = []\n",
        "          middle_list_dst = []\n",
        "          for j, word in enumerate(words) :\n",
        "\n",
        "            neighbors = words[max((j-q),0):j] + words[j:(j+q+1)]\n",
        "            neighbors_idx = [m for m in range(max((j-q), 0), max((j+q+1), len(neighbors)))]\n",
        "            dic_nodes_int_neighbors = []\n",
        "\n",
        "            for r, neigh in enumerate(neighbors):\n",
        "              if neigh in doubles : \n",
        "                dic_nodes_int_neighbors.append(neighbors_idx[r])\n",
        "              else :\n",
        "                dic_nodes_int_neighbors.append(dic_nodes_int[neigh])\n",
        "\n",
        "            middle_list_src = middle_list_src + [j for k in range(len(neighbors))] + dic_nodes_int_neighbors # Add edges in both directions\n",
        "            middle_list_dst = middle_list_dst + dic_nodes_int_neighbors + [j for k in range(len(neighbors))] #\n",
        "\n",
        "          #### TOP ####\n",
        "          top_list_src = []\n",
        "          top_list_dst = []\n",
        "          for j, word in enumerate(words) :\n",
        "            \n",
        "            top_list_src = top_list_src + [j for k in range(len(words))] + [p for p in range(len(words))] # Add edges in both directions\n",
        "            top_list_dst = top_list_dst + [p for p in range(len(words))] + [j for k in range(len(words))] #\n",
        "          \n",
        "\n",
        "          g = dgl.heterograph({ ('word', 'bottom', 'word'): (torch.tensor(bottom_list_src), torch.tensor(bottom_list_dst)), \n",
        "                                ('word', 'middle', 'word') : (torch.tensor(middle_list_src), torch.tensor(middle_list_src)), \n",
        "                                ('word', 'top', 'word'):(torch.tensor(top_list_src), torch.tensor(top_list_dst)) })\n",
        "          \n",
        "          \n",
        "      \n",
        "          features = torch.tensor(list_embeddings, dtype = torch.float32)\n",
        "         \n",
        "          g.ndata['word'] = features\n",
        "          self.graphs.append(g)\n",
        "        \n",
        "        \n",
        "        #Train/Val/Test/Split\n",
        "        num_examples = len(self.graphs)\n",
        "        num_train = int(0.80 * num_examples)\n",
        "        num_val = int((num_examples-num_train)/2)\n",
        "\n",
        "        self.graphs, self.labels = shuffle(self.graphs, self.labels, random_state = 2)\n",
        "        train_graphs, train_labels = self.graphs[:num_train], torch.LongTensor(self.labels[:num_train])\n",
        "        val_graphs, val_labels = self.graphs[num_train:(num_train+num_val)], torch.LongTensor(self.labels[num_train : (num_train+num_val)])\n",
        "        test_graphs, test_labels = self.graphs[(num_train+num_val) : (num_train+2*num_val)], torch.LongTensor(self.labels[(num_train+num_val):(num_train+2*num_val)])\n",
        "\n",
        "        # Convert the label list to tensor for saving.\n",
        "        #self.labels = torch.LongTensor(self.labels)\n",
        "        train_graph_path = ('/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_2.bin')\n",
        "        val_graph_path = ('/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_2.bin')\n",
        "        test_graph_path = ('/content/drive/My Drive/IASD_tmp/NLP/test_dgl_graph_2.bin')\n",
        "        \n",
        "        save_graphs(train_graph_path, train_graphs, {'labels': train_labels})\n",
        "        save_graphs(val_graph_path, val_graphs, {'labels': val_labels})\n",
        "        save_graphs(test_graph_path, test_graphs, {'labels': test_labels})\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graphs[i], self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "    def has_cache(self):\n",
        "        if os.path.exists(self.save_dir):\n",
        "          self.graphs, label_dict = load_graphs(self.save_dir)\n",
        "          self.labels = label_dict['labels']\n",
        "          return True"
      ],
      "metadata": {
        "id": "odes4cbBOiv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadGATLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, num_heads):\n",
        "        super(MultiHeadGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(num_heads):\n",
        "            self.heads.append(dgl.nn.pytorch.conv.GATConv(in_feats, in_feats, num_heads, allow_zero_in_degree=True))\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "    \n",
        "        head_outs = [attn_head(graph, feat) for attn_head in self.heads]\n",
        "        return torch.mean(torch.stack(head_outs))\n",
        "\n",
        "\n",
        "class MultiDotGATLayer(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, h_dot, num_heads):\n",
        "        super(MultiDotGATLayer, self).__init__()\n",
        "        self.heads = nn.ModuleList()\n",
        "        for i in range(h_dot):\n",
        "            self.heads.append(dglnn.conv.DotGatConv(in_feats, out_feats, num_heads))\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        head_outs = [attn_head(graph, feat) for attn_head in self.heads]\n",
        "        concat_feat = torch.stack(head_outs)\n",
        "        mean = torch.mean(concat_feat, dim = 0)\n",
        "        mean = torch.mean(mean, dim = 1)\n",
        "        return mean\n",
        "\n",
        "class BottomConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 beta,\n",
        "                 activation=None):\n",
        "        super(BottomConv, self).__init__()\n",
        "\n",
        "        self.gin = dgl.nn.pytorch.conv.GINConv(None, 'mean', init_eps=beta, learn_eps=True)\n",
        "        self.activation = activation\n",
        "        self.beta = beta\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        #graph.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_mean'))\n",
        "        #h_mean = graph.dstdata['h_mean']\n",
        "        #print(\"h_mean:\", h_mean)\n",
        "        #rst = torch.mul(h_mean, 1-self.beta) + torch.mul(feat, self.beta)\n",
        "        feat = feat[0]\n",
        "        h = self.gin(graph, feat)\n",
        "        h = self.linear(h)\n",
        "        h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MiddleConv(nn.Module):\n",
        "  def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 gamma,\n",
        "                 activation=None):\n",
        "        super(MiddleConv, self).__init__()\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        self.multigat = MultiHeadGATLayer(in_feats, in_feats, num_heads)\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "        self.activation = self.activation = activation\n",
        "\n",
        "  def forward(self, graph, feat):\n",
        "    feat = feat[0]\n",
        "    h = self.multigat(graph, feat)\n",
        "    h = torch.mul(h, 1-self.gamma) + torch.mul(feat, self.gamma)\n",
        "    h = self.linear(h)\n",
        "    h = self.activation(h)\n",
        "    return h\n",
        "\n",
        "class TopConv(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_feats,\n",
        "               out_feats, \n",
        "               h_dot, \n",
        "               num_heads, \n",
        "               activation = None):\n",
        "    super(TopConv, self).__init__()\n",
        "    self.multidotgat = MultiDotGATLayer(in_feats, out_feats, h_dot, num_heads)\n",
        "    self.linear = nn.Linear(out_feats, h_dot * out_feats, bias=False)\n",
        "    self.activation = activation\n",
        "  \n",
        "  def forward(self, graph, feat):\n",
        "    feat = feat[0]\n",
        "    h = self.multidotgat(graph, feat)\n",
        "    h = self.linear(h)\n",
        "    h = self.activation(h)\n",
        "    return h\n",
        "          \n",
        "class MLGNN(nn.Module):\n",
        "    def __init__(self, d0, d1, d2, d3, h_dot, beta, gamma, num_heads, c):\n",
        "        super().__init__()\n",
        "        self.d3 = d3\n",
        "        self.h_dot = h_dot\n",
        "        self.c = c\n",
        "\n",
        "        self.conv_bottom = dglnn.HeteroGraphConv({'bottom' : BottomConv(d0, d1, beta, nn.ReLU())}, aggregate='sum')\n",
        "        self.conv_middle = dglnn.HeteroGraphConv({'middle': MiddleConv(d1, d2, num_heads, gamma, nn.ReLU())}, aggregate='sum')\n",
        "        self.conv_top = dglnn.HeteroGraphConv({'top': TopConv(d2, d3, self.h_dot, num_heads, nn.ReLU())}, aggregate='sum')\n",
        "        self.linear = nn.Linear(self.h_dot * self.d3, self.c)\n",
        "        #self.classify = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, g):\n",
        "        bottom_g = g.edge_type_subgraph(['bottom'])\n",
        "        middle_g = g.edge_type_subgraph(['middle'])\n",
        "        top_g = g.edge_type_subgraph(['top'])\n",
        "\n",
        "        h = g.ndata\n",
        "        h = self.conv_bottom(bottom_g, h)\n",
        "        h = self.conv_middle(middle_g, h)\n",
        "        h = self.conv_top(top_g, h)\n",
        "        g.ndata['word'] = h['word']\n",
        "\n",
        "        h = dgl.readout_nodes(g, 'word', op='max')\n",
        "        h = self.linear(h)\n",
        "        #h = self.classify(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "2JjZUrk878lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(testloader, model):\n",
        "  num_correct = 0\n",
        "  num_tests = 0\n",
        "  for batched_graph, labels in testloader:\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    labels = labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      pred = model(batched_graph)\n",
        "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
        "    num_tests += len(labels)\n",
        "\n",
        "  return num_correct / num_tests"
      ],
      "metadata": {
        "id": "dg4D6bDqkOwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(testloader, model):\n",
        "  preds_proba = []\n",
        "  preds_labels = []\n",
        "  true_labels = []\n",
        "  for batched_graph, labels in testloader:\n",
        "    true_labels.append(labels)\n",
        "    batched_graph = batched_graph.to(device)\n",
        "    labels = labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      pred = model(batched_graph)\n",
        "      preds_proba.append(pred)\n",
        "\n",
        "      _, pred_labels = torch.max(pred, 1)\n",
        "      preds_labels.append(pred_labels)\n",
        "\n",
        "  preds_proba = torch.cat(preds_proba, dim=0)\n",
        "  preds_labels = torch.cat(preds_labels, dim=0)\n",
        "  true_labels = torch.cat(true_labels, dim=0)\n",
        "  return preds_proba, preds_labels, true_labels"
      ],
      "metadata": {
        "id": "V3ns71muXD3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "kL-92fD2dHDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the hyperparameter search on several cells, to prevent a cell stops running just before its end, due to Colab limitations."
      ],
      "metadata": {
        "id": "krQCKmB8LxxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d0 = 768\n",
        "c = 6"
      ],
      "metadata": {
        "id": "dxpK0wtGKiU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':1/2, 'gamma':1/2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001},\n",
        "                      {'d1':816, 'd2':912, 'd3':700, 'h_dot':2, 'num_heads':2, 'beta':1/2, 'gamma':1/2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001},\n",
        "                      {'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':1/3, 'gamma':1/3, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001}]\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "id": "9c6RlBCWJs2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b684ef-9359-4263-a03f-db2a6f9edf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.1313\n",
            "Epoch 1, loss 0.8726\n",
            "Epoch 2, loss 0.8381\n",
            "Epoch 3, loss 0.8255\n",
            "Epoch 4, loss 0.8172\n",
            "Epoch 5, loss 0.8093\n",
            "Epoch 6, loss 0.7659\n",
            "Epoch 7, loss 0.6666\n",
            "Epoch 8, loss 0.5986\n",
            "Epoch 9, loss 0.5737\n",
            "Epoch 10, loss 0.6245\n",
            "Epoch 11, loss 0.6010\n",
            "Epoch 12, loss 0.5677\n",
            "Epoch 13, loss 0.5568\n",
            "Epoch 14, loss 0.7753\n",
            "Epoch 15, loss 0.5849\n",
            "Epoch 16, loss 0.5248\n",
            "Epoch 17, loss 0.5013\n",
            "Epoch 18, loss 0.6642\n",
            "Epoch 19, loss 0.6376\n",
            "Epoch 0, loss 1.1550\n",
            "Epoch 1, loss 0.8794\n",
            "Epoch 2, loss 0.8370\n",
            "Epoch 3, loss 0.8202\n",
            "Epoch 4, loss 0.7999\n",
            "Epoch 5, loss 0.7379\n",
            "Epoch 6, loss 0.6653\n",
            "Epoch 7, loss 0.6746\n",
            "Epoch 8, loss 0.6087\n",
            "Epoch 9, loss 0.5805\n",
            "Epoch 10, loss 0.5440\n",
            "Epoch 11, loss 0.5161\n",
            "Epoch 12, loss 0.5037\n",
            "Epoch 13, loss 0.5080\n",
            "Epoch 14, loss 0.5088\n",
            "Epoch 15, loss 0.5132\n",
            "Epoch 16, loss 0.4795\n",
            "Epoch 17, loss 0.4906\n",
            "Epoch 18, loss 0.4744\n",
            "Epoch 19, loss 0.4686\n",
            "Epoch 0, loss 1.1361\n",
            "Epoch 1, loss 0.8473\n",
            "Epoch 2, loss 0.8341\n",
            "Epoch 3, loss 0.8273\n",
            "Epoch 4, loss 0.8301\n",
            "Epoch 5, loss 0.8419\n",
            "Epoch 6, loss 0.8143\n",
            "Epoch 7, loss 0.8164\n",
            "Epoch 8, loss 0.7392\n",
            "Epoch 9, loss 0.6429\n",
            "Epoch 10, loss 0.5833\n",
            "Epoch 11, loss 0.5617\n",
            "Epoch 12, loss 0.5631\n",
            "Epoch 13, loss 0.7053\n",
            "Epoch 14, loss 0.7947\n",
            "Epoch 15, loss 0.8255\n",
            "Epoch 16, loss 0.6231\n",
            "Epoch 17, loss 0.5120\n",
            "Epoch 18, loss 0.4985\n",
            "Epoch 19, loss 0.5032\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.7836666666666666 with std: 0.0330891052899423\n",
            "Validation f1-score: 0.7606188881746548 with std: 0.03749550081170252\n",
            "Validation auc: 0.6931839795569892 with std: 0.06271268832279887\n",
            "Test acc: 0.7836666666666666 with std: 0.0330891052899423\n",
            "Test f1-score: 0.7606188881746548 with std: 0.03749550081170252\n",
            "Test auc: 0.6931839795569892 with std: 0.06271268832279887\n",
            "Epoch 0, loss 1.1478\n",
            "Epoch 1, loss 0.8928\n",
            "Epoch 2, loss 0.8318\n",
            "Epoch 3, loss 0.8111\n",
            "Epoch 4, loss 0.7730\n",
            "Epoch 5, loss 0.6851\n",
            "Epoch 6, loss 0.6267\n",
            "Epoch 7, loss 0.6539\n",
            "Epoch 8, loss 0.6285\n",
            "Epoch 9, loss 0.5898\n",
            "Epoch 10, loss 0.5483\n",
            "Epoch 11, loss 0.5390\n",
            "Epoch 12, loss 0.5564\n",
            "Epoch 13, loss 0.5487\n",
            "Epoch 14, loss 0.5672\n",
            "Epoch 15, loss 0.5331\n",
            "Epoch 16, loss 0.5110\n",
            "Epoch 17, loss 0.5015\n",
            "Epoch 18, loss 0.5034\n",
            "Epoch 19, loss 0.4957\n",
            "Epoch 0, loss 1.1289\n",
            "Epoch 1, loss 0.8663\n",
            "Epoch 2, loss 0.8269\n",
            "Epoch 3, loss 0.8174\n",
            "Epoch 4, loss 0.7971\n",
            "Epoch 5, loss 0.7474\n",
            "Epoch 6, loss 0.6596\n",
            "Epoch 7, loss 0.6212\n",
            "Epoch 8, loss 0.6076\n",
            "Epoch 9, loss 0.5587\n",
            "Epoch 10, loss 0.5493\n",
            "Epoch 11, loss 0.5742\n",
            "Epoch 12, loss 0.5337\n",
            "Epoch 13, loss 0.5994\n",
            "Epoch 14, loss 0.5833\n",
            "Epoch 15, loss 0.6144\n",
            "Epoch 16, loss 0.6287\n",
            "Epoch 17, loss 0.8759\n",
            "Epoch 18, loss 0.8199\n",
            "Epoch 19, loss 0.7968\n",
            "Epoch 0, loss 1.1590\n",
            "Epoch 1, loss 0.8999\n",
            "Epoch 2, loss 0.8262\n",
            "Epoch 3, loss 0.8188\n",
            "Epoch 4, loss 0.8096\n",
            "Epoch 5, loss 0.7714\n",
            "Epoch 6, loss 0.7548\n",
            "Epoch 7, loss 0.6717\n",
            "Epoch 8, loss 0.5781\n",
            "Epoch 9, loss 0.5463\n",
            "Epoch 10, loss 0.5219\n",
            "Epoch 11, loss 0.5148\n",
            "Epoch 12, loss 0.5493\n",
            "Epoch 13, loss 0.5071\n",
            "Epoch 14, loss 0.4946\n",
            "Epoch 15, loss 0.4785\n",
            "Epoch 16, loss 0.4814\n",
            "Epoch 17, loss 0.4707\n",
            "Epoch 18, loss 0.4700\n",
            "Epoch 19, loss 0.4848\n",
            "Metrics for  1 combination:\n",
            "\n",
            "Validation acc: 0.6930000000000001 with std: 0.1379154330257012\n",
            "Validation f1-score: 0.6215807333351263 with std: 0.20542589637486292\n",
            "Validation auc: 0.6791753917195055 with std: 0.06708641106252985\n",
            "Test acc: 0.6930000000000001 with std: 0.1379154330257012\n",
            "Test f1-score: 0.6215807333351263 with std: 0.20542589637486292\n",
            "Test auc: 0.6791753917195055 with std: 0.06708641106252985\n",
            "Epoch 0, loss 1.1159\n",
            "Epoch 1, loss 0.8699\n",
            "Epoch 2, loss 0.8265\n",
            "Epoch 3, loss 0.8220\n",
            "Epoch 4, loss 0.8214\n",
            "Epoch 5, loss 0.8178\n",
            "Epoch 6, loss 0.8155\n",
            "Epoch 7, loss 0.8080\n",
            "Epoch 8, loss 0.7732\n",
            "Epoch 9, loss 0.7644\n",
            "Epoch 10, loss 0.7042\n",
            "Epoch 11, loss 0.6472\n",
            "Epoch 12, loss 0.6848\n",
            "Epoch 13, loss 0.5813\n",
            "Epoch 14, loss 0.5854\n",
            "Epoch 15, loss 0.5609\n",
            "Epoch 16, loss 0.5263\n",
            "Epoch 17, loss 0.5261\n",
            "Epoch 18, loss 0.5152\n",
            "Epoch 19, loss 0.5105\n",
            "Epoch 0, loss 1.1445\n",
            "Epoch 1, loss 0.8850\n",
            "Epoch 2, loss 0.8316\n",
            "Epoch 3, loss 0.8208\n",
            "Epoch 4, loss 0.8119\n",
            "Epoch 5, loss 0.7966\n",
            "Epoch 6, loss 0.7538\n",
            "Epoch 7, loss 0.7047\n",
            "Epoch 8, loss 0.6448\n",
            "Epoch 9, loss 0.9519\n",
            "Epoch 10, loss 1.4846\n",
            "Epoch 11, loss 0.8651\n",
            "Epoch 12, loss 0.7575\n",
            "Epoch 13, loss 0.6459\n",
            "Epoch 14, loss 0.5638\n",
            "Epoch 15, loss 0.6050\n",
            "Epoch 16, loss 0.5410\n",
            "Epoch 17, loss 0.5105\n",
            "Epoch 18, loss 0.6391\n",
            "Epoch 19, loss 0.9547\n",
            "Epoch 0, loss 1.0930\n",
            "Epoch 1, loss 0.8627\n",
            "Epoch 2, loss 0.8288\n",
            "Epoch 3, loss 0.8181\n",
            "Epoch 4, loss 0.8150\n",
            "Epoch 5, loss 0.8078\n",
            "Epoch 6, loss 0.7907\n",
            "Epoch 7, loss 0.7647\n",
            "Epoch 8, loss 0.7761\n",
            "Epoch 9, loss 0.6655\n",
            "Epoch 10, loss 0.6021\n",
            "Epoch 11, loss 0.5608\n",
            "Epoch 12, loss 0.7758\n",
            "Epoch 13, loss 0.6472\n",
            "Epoch 14, loss 0.6052\n",
            "Epoch 15, loss 0.5497\n",
            "Epoch 16, loss 0.5326\n",
            "Epoch 17, loss 0.5284\n",
            "Epoch 18, loss 0.6073\n",
            "Epoch 19, loss 0.5452\n",
            "Metrics for  2 combination:\n",
            "\n",
            "Validation acc: 0.749 with std: 0.06083310502240262\n",
            "Validation f1-score: 0.7207490354336351 with std: 0.06673234924047627\n",
            "Validation auc: 0.7216214184106587 with std: 0.022143994225987503\n",
            "Test acc: 0.749 with std: 0.06083310502240262\n",
            "Test f1-score: 0.7207490354336351 with std: 0.06673234924047627\n",
            "Test auc: 0.7216214184106587 with std: 0.022143994225987503\n",
            "Best hyperparameters combination: 0\n",
            "Test accuracy: 0.7836666666666666 0.0330891052899423\n",
            "Test f1-score: 0.7606188881746548 0.03749550081170252\n",
            "Test auc: 0.6931839795569892 0.06271268832279887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':912, 'd2':1024, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':1/2, 'gamma':1/2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001},\n",
        "                      {'d1':912, 'd2':1024, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':2/3, 'gamma':2/3, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.0001}]\n",
        "\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0xBObCrud20",
        "outputId": "17995831-aa52-4689-cf99-d281b9a3a067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.1978\n",
            "Epoch 1, loss 0.8630\n",
            "Epoch 2, loss 0.8266\n",
            "Epoch 3, loss 0.8173\n",
            "Epoch 4, loss 0.7870\n",
            "Epoch 5, loss 0.8059\n",
            "Epoch 6, loss 0.7569\n",
            "Epoch 7, loss 0.6556\n",
            "Epoch 8, loss 0.6134\n",
            "Epoch 9, loss 0.5802\n",
            "Epoch 10, loss 0.5541\n",
            "Epoch 11, loss 0.5188\n",
            "Epoch 12, loss 0.5138\n",
            "Epoch 13, loss 0.5189\n",
            "Epoch 14, loss 0.5064\n",
            "Epoch 15, loss 0.5176\n",
            "Epoch 16, loss 0.5235\n",
            "Epoch 17, loss 0.5337\n",
            "Epoch 18, loss 0.5284\n",
            "Epoch 19, loss 0.5154\n",
            "Epoch 0, loss 1.1177\n",
            "Epoch 1, loss 0.8614\n",
            "Epoch 2, loss 0.8242\n",
            "Epoch 3, loss 0.8219\n",
            "Epoch 4, loss 0.8172\n",
            "Epoch 5, loss 0.8055\n",
            "Epoch 6, loss 0.7628\n",
            "Epoch 7, loss 0.8301\n",
            "Epoch 8, loss 0.7130\n",
            "Epoch 9, loss 0.6029\n",
            "Epoch 10, loss 0.6818\n",
            "Epoch 11, loss 0.6373\n",
            "Epoch 12, loss 0.5815\n",
            "Epoch 13, loss 0.6031\n",
            "Epoch 14, loss 0.5289\n",
            "Epoch 15, loss 0.5174\n",
            "Epoch 16, loss 0.5021\n",
            "Epoch 17, loss 0.5026\n",
            "Epoch 18, loss 0.4948\n",
            "Epoch 19, loss 0.4851\n",
            "Epoch 0, loss 1.1256\n",
            "Epoch 1, loss 0.8464\n",
            "Epoch 2, loss 0.8255\n",
            "Epoch 3, loss 0.8200\n",
            "Epoch 4, loss 0.8107\n",
            "Epoch 5, loss 0.7906\n",
            "Epoch 6, loss 0.7046\n",
            "Epoch 7, loss 0.6909\n",
            "Epoch 8, loss 0.6193\n",
            "Epoch 9, loss 0.5476\n",
            "Epoch 10, loss 0.5213\n",
            "Epoch 11, loss 0.5514\n",
            "Epoch 12, loss 0.5657\n",
            "Epoch 13, loss 0.5539\n",
            "Epoch 14, loss 0.5274\n",
            "Epoch 15, loss 0.5202\n",
            "Epoch 16, loss 0.5127\n",
            "Epoch 17, loss 0.5376\n",
            "Epoch 18, loss 0.5061\n",
            "Epoch 19, loss 0.5434\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.7716666666666668 with std: 0.06094988834189022\n",
            "Validation f1-score: 0.7456454030944902 with std: 0.06767191073169541\n",
            "Validation auc: 0.7240225583623873 with std: 0.0043116620833495035\n",
            "Test acc: 0.7716666666666668 with std: 0.06094988834189022\n",
            "Test f1-score: 0.7456454030944902 with std: 0.06767191073169541\n",
            "Test auc: 0.7240225583623873 with std: 0.0043116620833495035\n",
            "Epoch 0, loss 1.3955\n",
            "Epoch 1, loss 0.9070\n",
            "Epoch 2, loss 0.8851\n",
            "Epoch 3, loss 0.8286\n",
            "Epoch 4, loss 0.8306\n",
            "Epoch 5, loss 0.8239\n",
            "Epoch 6, loss 0.8196\n",
            "Epoch 7, loss 0.8186\n",
            "Epoch 8, loss 0.8149\n",
            "Epoch 9, loss 0.8100\n",
            "Epoch 10, loss 0.8049\n",
            "Epoch 11, loss 0.7968\n",
            "Epoch 12, loss 0.7860\n",
            "Epoch 13, loss 0.7765\n",
            "Epoch 14, loss 0.7689\n",
            "Epoch 15, loss 0.7583\n",
            "Epoch 16, loss 0.7397\n",
            "Epoch 17, loss 0.7207\n",
            "Epoch 18, loss 0.7067\n",
            "Epoch 19, loss 0.6938\n",
            "Epoch 0, loss 1.3665\n",
            "Epoch 1, loss 0.8872\n",
            "Epoch 2, loss 0.8781\n",
            "Epoch 3, loss 0.8285\n",
            "Epoch 4, loss 0.8247\n",
            "Epoch 5, loss 0.8210\n",
            "Epoch 6, loss 0.8156\n",
            "Epoch 7, loss 0.8120\n",
            "Epoch 8, loss 0.8061\n",
            "Epoch 9, loss 0.7978\n",
            "Epoch 10, loss 0.7869\n",
            "Epoch 11, loss 0.7738\n",
            "Epoch 12, loss 0.7631\n",
            "Epoch 13, loss 0.7527\n",
            "Epoch 14, loss 0.7367\n",
            "Epoch 15, loss 0.7255\n",
            "Epoch 16, loss 0.7092\n",
            "Epoch 17, loss 0.6872\n",
            "Epoch 18, loss 0.6735\n",
            "Epoch 19, loss 0.6629\n",
            "Epoch 0, loss 1.3972\n",
            "Epoch 1, loss 0.8982\n",
            "Epoch 2, loss 0.8880\n",
            "Epoch 3, loss 0.8299\n",
            "Epoch 4, loss 0.8313\n",
            "Epoch 5, loss 0.8219\n",
            "Epoch 6, loss 0.8205\n",
            "Epoch 7, loss 0.8178\n",
            "Epoch 8, loss 0.8136\n",
            "Epoch 9, loss 0.8083\n",
            "Epoch 10, loss 0.8006\n",
            "Epoch 11, loss 0.7905\n",
            "Epoch 12, loss 0.7770\n",
            "Epoch 13, loss 0.7621\n",
            "Epoch 14, loss 0.7441\n",
            "Epoch 15, loss 0.7258\n",
            "Epoch 16, loss 0.6994\n",
            "Epoch 17, loss 0.6822\n",
            "Epoch 18, loss 0.6653\n",
            "Epoch 19, loss 0.6615\n",
            "Metrics for  1 combination:\n",
            "\n",
            "Validation acc: 0.7223333333333333 with std: 0.02075786330258704\n",
            "Validation f1-score: 0.7017476870336203 with std: 0.021684408368093124\n",
            "Validation auc: 0.6160796547790034 with std: 0.02633347769342445\n",
            "Test acc: 0.7223333333333333 with std: 0.02075786330258704\n",
            "Test f1-score: 0.7017476870336203 with std: 0.021684408368093124\n",
            "Test auc: 0.6160796547790034 with std: 0.02633347769342445\n",
            "Best hyperparameters combination: 0\n",
            "Test accuracy: 0.7716666666666668 0.06094988834189022\n",
            "Test f1-score: 0.7456454030944902 0.06767191073169541\n",
            "Test auc: 0.7240225583623873 0.0043116620833495035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':3, 'beta':1/2, 'gamma':1/2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001},\n",
        "                      {'d1':816, 'd2':912, 'd3':1024, 'h_dot':3, 'num_heads':2, 'beta':1/2, 'gamma':1/2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001}]\n",
        "\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyjIxOpt-usa",
        "outputId": "126f2323-7e77-4b52-e9bd-431f1af2fa30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.2142\n",
            "Epoch 1, loss 0.9270\n",
            "Epoch 2, loss 0.8251\n",
            "Epoch 3, loss 0.8170\n",
            "Epoch 4, loss 0.8151\n",
            "Epoch 5, loss 0.7675\n",
            "Epoch 6, loss 0.9212\n",
            "Epoch 7, loss 0.9060\n",
            "Epoch 8, loss 0.7188\n",
            "Epoch 9, loss 0.6361\n",
            "Epoch 10, loss 0.6037\n",
            "Epoch 11, loss 0.6249\n",
            "Epoch 12, loss 0.5927\n",
            "Epoch 13, loss 0.5339\n",
            "Epoch 14, loss 0.5252\n",
            "Epoch 15, loss 0.5155\n",
            "Epoch 16, loss 0.5381\n",
            "Epoch 17, loss 0.6479\n",
            "Epoch 18, loss 0.5890\n",
            "Epoch 19, loss 0.5607\n",
            "Epoch 0, loss 1.1809\n",
            "Epoch 1, loss 0.8937\n",
            "Epoch 2, loss 0.8332\n",
            "Epoch 3, loss 0.8235\n",
            "Epoch 4, loss 0.8194\n",
            "Epoch 5, loss 0.8138\n",
            "Epoch 6, loss 0.7984\n",
            "Epoch 7, loss 0.7180\n",
            "Epoch 8, loss 0.7936\n",
            "Epoch 9, loss 0.6755\n",
            "Epoch 10, loss 0.5823\n",
            "Epoch 11, loss 0.5521\n",
            "Epoch 12, loss 0.5273\n",
            "Epoch 13, loss 0.5110\n",
            "Epoch 14, loss 0.4891\n",
            "Epoch 15, loss 0.4745\n",
            "Epoch 16, loss 0.5255\n",
            "Epoch 17, loss 0.5113\n",
            "Epoch 18, loss 0.4997\n",
            "Epoch 19, loss 1.4227\n",
            "Epoch 0, loss 1.1562\n",
            "Epoch 1, loss 0.8599\n",
            "Epoch 2, loss 0.8277\n",
            "Epoch 3, loss 0.8220\n",
            "Epoch 4, loss 0.8180\n",
            "Epoch 5, loss 0.8065\n",
            "Epoch 6, loss 0.7506\n",
            "Epoch 7, loss 0.8159\n",
            "Epoch 8, loss 0.7189\n",
            "Epoch 9, loss 0.6111\n",
            "Epoch 10, loss 0.5843\n",
            "Epoch 11, loss 0.5716\n",
            "Epoch 12, loss 0.5393\n",
            "Epoch 13, loss 0.6730\n",
            "Epoch 14, loss 0.6487\n",
            "Epoch 15, loss 0.6012\n",
            "Epoch 16, loss 0.5352\n",
            "Epoch 17, loss 0.5132\n",
            "Epoch 18, loss 0.5202\n",
            "Epoch 19, loss 0.5432\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.6876666666666668 with std: 0.13414502931114852\n",
            "Validation f1-score: 0.6153056757225149 with std: 0.20100500878623387\n",
            "Validation auc: 0.6391058340949437 with std: 0.11539271128173544\n",
            "Test acc: 0.6876666666666668 with std: 0.13414502931114852\n",
            "Test f1-score: 0.6153056757225149 with std: 0.20100500878623387\n",
            "Test auc: 0.6391058340949437 with std: 0.11539271128173544\n",
            "Epoch 0, loss 1.1612\n",
            "Epoch 1, loss 0.8751\n",
            "Epoch 2, loss 0.8275\n",
            "Epoch 3, loss 0.8170\n",
            "Epoch 4, loss 0.8176\n",
            "Epoch 5, loss 0.8194\n",
            "Epoch 6, loss 0.8191\n",
            "Epoch 7, loss 0.8086\n",
            "Epoch 8, loss 0.7745\n",
            "Epoch 9, loss 0.7035\n",
            "Epoch 10, loss 0.6132\n",
            "Epoch 11, loss 0.6086\n",
            "Epoch 12, loss 0.5801\n",
            "Epoch 13, loss 0.5330\n",
            "Epoch 14, loss 0.5780\n",
            "Epoch 15, loss 0.6679\n",
            "Epoch 16, loss 0.5884\n",
            "Epoch 17, loss 0.5402\n",
            "Epoch 18, loss 0.5326\n",
            "Epoch 19, loss 0.4959\n",
            "Epoch 0, loss 1.1677\n",
            "Epoch 1, loss 0.8522\n",
            "Epoch 2, loss 0.8318\n",
            "Epoch 3, loss 0.8032\n",
            "Epoch 4, loss 0.7684\n",
            "Epoch 5, loss 0.7298\n",
            "Epoch 6, loss 0.6442\n",
            "Epoch 7, loss 0.5801\n",
            "Epoch 8, loss 0.5577\n",
            "Epoch 9, loss 0.5537\n",
            "Epoch 10, loss 0.5436\n",
            "Epoch 11, loss 0.5236\n",
            "Epoch 12, loss 0.5196\n",
            "Epoch 13, loss 0.5304\n",
            "Epoch 14, loss 0.5139\n",
            "Epoch 15, loss 0.5020\n",
            "Epoch 16, loss 0.5455\n",
            "Epoch 17, loss 0.5118\n",
            "Epoch 18, loss 0.4811\n",
            "Epoch 19, loss 0.4776\n",
            "Epoch 0, loss 1.1199\n",
            "Epoch 1, loss 0.8413\n",
            "Epoch 2, loss 0.8370\n",
            "Epoch 3, loss 0.8119\n",
            "Epoch 4, loss 0.7782\n",
            "Epoch 5, loss 0.8192\n",
            "Epoch 6, loss 0.7215\n",
            "Epoch 7, loss 0.6122\n",
            "Epoch 8, loss 0.6466\n",
            "Epoch 9, loss 0.6245\n",
            "Epoch 10, loss 0.6248\n",
            "Epoch 11, loss 0.5661\n",
            "Epoch 12, loss 0.5388\n",
            "Epoch 13, loss 0.5236\n",
            "Epoch 14, loss 0.4974\n",
            "Epoch 15, loss 0.4835\n",
            "Epoch 16, loss 0.4760\n",
            "Epoch 17, loss 0.4930\n",
            "Epoch 18, loss 0.4888\n",
            "Epoch 19, loss 0.5370\n",
            "Metrics for  1 combination:\n",
            "\n",
            "Validation acc: 0.7503333333333333 with std: 0.09491867864417179\n",
            "Validation f1-score: 0.7160002238529527 with std: 0.11321545274817266\n",
            "Validation auc: 0.7256559286532832 with std: 0.01887105742966569\n",
            "Test acc: 0.7503333333333333 with std: 0.09491867864417179\n",
            "Test f1-score: 0.7160002238529527 with std: 0.11321545274817266\n",
            "Test auc: 0.7256559286532832 with std: 0.01887105742966569\n",
            "Best hyperparameters combination: 1\n",
            "Test accuracy: 0.7503333333333333 0.09491867864417179\n",
            "Test f1-score: 0.7160002238529527 0.11321545274817266\n",
            "Test auc: 0.7256559286532832 0.01887105742966569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':1, 'gamma':1, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001},\n",
        "                      {'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':2/3, 'gamma':2/3, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001}]\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAB0PVEU07CO",
        "outputId": "9b9c5bc6-875e-49e8-ea80-0983e41377ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.2130\n",
            "Epoch 1, loss 0.9125\n",
            "Epoch 2, loss 0.8331\n",
            "Epoch 3, loss 0.8152\n",
            "Epoch 4, loss 0.7722\n",
            "Epoch 5, loss 0.7126\n",
            "Epoch 6, loss 0.6483\n",
            "Epoch 7, loss 0.6354\n",
            "Epoch 8, loss 0.5774\n",
            "Epoch 9, loss 0.5423\n",
            "Epoch 10, loss 0.5485\n",
            "Epoch 11, loss 0.5144\n",
            "Epoch 12, loss 0.5074\n",
            "Epoch 13, loss 0.5349\n",
            "Epoch 14, loss 0.5331\n",
            "Epoch 15, loss 0.5729\n",
            "Epoch 16, loss 0.5713\n",
            "Epoch 17, loss 0.5685\n",
            "Epoch 18, loss 0.5465\n",
            "Epoch 19, loss 0.5112\n",
            "Epoch 0, loss 1.2776\n",
            "Epoch 1, loss 0.9291\n",
            "Epoch 2, loss 0.8449\n",
            "Epoch 3, loss 0.8053\n",
            "Epoch 4, loss 0.7479\n",
            "Epoch 5, loss 0.6771\n",
            "Epoch 6, loss 0.6296\n",
            "Epoch 7, loss 0.6722\n",
            "Epoch 8, loss 0.6579\n",
            "Epoch 9, loss 0.5764\n",
            "Epoch 10, loss 0.5470\n",
            "Epoch 11, loss 0.5606\n",
            "Epoch 12, loss 0.5452\n",
            "Epoch 13, loss 0.5112\n",
            "Epoch 14, loss 0.5869\n",
            "Epoch 15, loss 0.8927\n",
            "Epoch 16, loss 0.7710\n",
            "Epoch 17, loss 0.6823\n",
            "Epoch 18, loss 0.6740\n",
            "Epoch 19, loss 0.5880\n",
            "Epoch 0, loss 1.2259\n",
            "Epoch 1, loss 0.9230\n",
            "Epoch 2, loss 0.8485\n",
            "Epoch 3, loss 0.8084\n",
            "Epoch 4, loss 0.7179\n",
            "Epoch 5, loss 0.6706\n",
            "Epoch 6, loss 0.5921\n",
            "Epoch 7, loss 0.5853\n",
            "Epoch 8, loss 0.5553\n",
            "Epoch 9, loss 0.5303\n",
            "Epoch 10, loss 0.5021\n",
            "Epoch 11, loss 0.5068\n",
            "Epoch 12, loss 0.4939\n",
            "Epoch 13, loss 0.4918\n",
            "Epoch 14, loss 0.4798\n",
            "Epoch 15, loss 0.4785\n",
            "Epoch 16, loss 0.4867\n",
            "Epoch 17, loss 0.4906\n",
            "Epoch 18, loss 0.4935\n",
            "Epoch 19, loss 0.4994\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.8006666666666667 with std: 0.00654896090146284\n",
            "Validation f1-score: 0.778720774272997 with std: 0.0050584778370795095\n",
            "Validation auc: 0.7166330759305053 with std: 0.01826603443414899\n",
            "Test acc: 0.8006666666666667 with std: 0.00654896090146284\n",
            "Test f1-score: 0.778720774272997 with std: 0.0050584778370795095\n",
            "Test auc: 0.7166330759305053 with std: 0.01826603443414899\n",
            "Epoch 0, loss 1.1502\n",
            "Epoch 1, loss 0.8830\n",
            "Epoch 2, loss 0.8567\n",
            "Epoch 3, loss 0.8284\n",
            "Epoch 4, loss 0.8201\n",
            "Epoch 5, loss 0.8127\n",
            "Epoch 6, loss 0.8033\n",
            "Epoch 7, loss 0.7879\n",
            "Epoch 8, loss 0.7803\n",
            "Epoch 9, loss 0.6826\n",
            "Epoch 10, loss 0.5962\n",
            "Epoch 11, loss 0.5967\n",
            "Epoch 12, loss 0.5627\n",
            "Epoch 13, loss 0.5689\n",
            "Epoch 14, loss 0.5551\n",
            "Epoch 15, loss 0.5608\n",
            "Epoch 16, loss 0.5222\n",
            "Epoch 17, loss 0.5386\n",
            "Epoch 18, loss 0.5139\n",
            "Epoch 19, loss 0.5157\n",
            "Epoch 0, loss 1.1860\n",
            "Epoch 1, loss 0.8992\n",
            "Epoch 2, loss 0.8385\n",
            "Epoch 3, loss 0.8704\n",
            "Epoch 4, loss 0.8376\n",
            "Epoch 5, loss 0.7791\n",
            "Epoch 6, loss 0.7332\n",
            "Epoch 7, loss 0.6817\n",
            "Epoch 8, loss 0.6341\n",
            "Epoch 9, loss 0.6005\n",
            "Epoch 10, loss 0.5748\n",
            "Epoch 11, loss 0.5496\n",
            "Epoch 12, loss 0.5308\n",
            "Epoch 13, loss 0.5250\n",
            "Epoch 14, loss 0.9242\n",
            "Epoch 15, loss 0.8018\n",
            "Epoch 16, loss 0.6319\n",
            "Epoch 17, loss 0.5757\n",
            "Epoch 18, loss 0.5593\n",
            "Epoch 19, loss 0.5367\n",
            "Epoch 0, loss 1.1405\n",
            "Epoch 1, loss 0.8646\n",
            "Epoch 2, loss 0.8298\n",
            "Epoch 3, loss 0.8009\n",
            "Epoch 4, loss 0.7612\n",
            "Epoch 5, loss 0.7275\n",
            "Epoch 6, loss 0.6403\n",
            "Epoch 7, loss 0.6143\n",
            "Epoch 8, loss 0.5942\n",
            "Epoch 9, loss 0.5716\n",
            "Epoch 10, loss 0.5246\n",
            "Epoch 11, loss 0.5298\n",
            "Epoch 12, loss 0.5152\n",
            "Epoch 13, loss 0.5556\n",
            "Epoch 14, loss 0.5690\n",
            "Epoch 15, loss 0.5610\n",
            "Epoch 16, loss 0.5342\n",
            "Epoch 17, loss 0.5225\n",
            "Epoch 18, loss 0.5008\n",
            "Epoch 19, loss 0.4916\n",
            "Metrics for  1 combination:\n",
            "\n",
            "Validation acc: 0.761 with std: 0.06772001181334804\n",
            "Validation f1-score: 0.7369044184072334 with std: 0.07321254452926976\n",
            "Validation auc: 0.7247372940901874 with std: 0.0018801978833065018\n",
            "Test acc: 0.761 with std: 0.06772001181334804\n",
            "Test f1-score: 0.7369044184072334 with std: 0.07321254452926976\n",
            "Test auc: 0.7247372940901874 with std: 0.0018801978833065018\n",
            "Best hyperparameters combination: 0\n",
            "Test accuracy: 0.8006666666666667 0.00654896090146284\n",
            "Test f1-score: 0.778720774272997 0.0050584778370795095\n",
            "Test auc: 0.7166330759305053 0.01826603443414899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':2, 'gamma':2, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001}]\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PECiq8zAFyiX",
        "outputId": "55a8912b-06ad-4dc8-84b2-36e7870acef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.5302\n",
            "Epoch 1, loss 0.8903\n",
            "Epoch 2, loss 0.8535\n",
            "Epoch 3, loss 0.8301\n",
            "Epoch 4, loss 0.8193\n",
            "Epoch 5, loss 0.8075\n",
            "Epoch 6, loss 0.7877\n",
            "Epoch 7, loss 0.7468\n",
            "Epoch 8, loss 0.7081\n",
            "Epoch 9, loss 0.6653\n",
            "Epoch 10, loss 0.6051\n",
            "Epoch 11, loss 0.7274\n",
            "Epoch 12, loss 0.6981\n",
            "Epoch 13, loss 0.6042\n",
            "Epoch 14, loss 0.5626\n",
            "Epoch 15, loss 0.5430\n",
            "Epoch 16, loss 0.6204\n",
            "Epoch 17, loss 0.5675\n",
            "Epoch 18, loss 0.5369\n",
            "Epoch 19, loss 0.5199\n",
            "Epoch 0, loss 1.5320\n",
            "Epoch 1, loss 0.8831\n",
            "Epoch 2, loss 0.8227\n",
            "Epoch 3, loss 0.7736\n",
            "Epoch 4, loss 0.7237\n",
            "Epoch 5, loss 0.6037\n",
            "Epoch 6, loss 0.5736\n",
            "Epoch 7, loss 0.5814\n",
            "Epoch 8, loss 0.6383\n",
            "Epoch 9, loss 0.7668\n",
            "Epoch 10, loss 0.6805\n",
            "Epoch 11, loss 0.6021\n",
            "Epoch 12, loss 0.5772\n",
            "Epoch 13, loss 0.5316\n",
            "Epoch 14, loss 0.5289\n",
            "Epoch 15, loss 0.5134\n",
            "Epoch 16, loss 0.5982\n",
            "Epoch 17, loss 0.6326\n",
            "Epoch 18, loss 0.7511\n",
            "Epoch 19, loss 0.8123\n",
            "Epoch 0, loss 1.5110\n",
            "Epoch 1, loss 0.8647\n",
            "Epoch 2, loss 0.8349\n",
            "Epoch 3, loss 0.8350\n",
            "Epoch 4, loss 0.8223\n",
            "Epoch 5, loss 0.7820\n",
            "Epoch 6, loss 0.6995\n",
            "Epoch 7, loss 0.6248\n",
            "Epoch 8, loss 0.7035\n",
            "Epoch 9, loss 0.6808\n",
            "Epoch 10, loss 0.6139\n",
            "Epoch 11, loss 0.5935\n",
            "Epoch 12, loss 0.5375\n",
            "Epoch 13, loss 0.5429\n",
            "Epoch 14, loss 0.5665\n",
            "Epoch 15, loss 0.5408\n",
            "Epoch 16, loss 0.5395\n",
            "Epoch 17, loss 0.5190\n",
            "Epoch 18, loss 0.5278\n",
            "Epoch 19, loss 0.5239\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.7586666666666666 with std: 0.046147107770211984\n",
            "Validation f1-score: 0.7343396686555153 with std: 0.04714610589450773\n",
            "Validation auc: 0.695924800583484 with std: 0.051212978879204014\n",
            "Test acc: 0.7586666666666666 with std: 0.046147107770211984\n",
            "Test f1-score: 0.7343396686555153 with std: 0.04714610589450773\n",
            "Test auc: 0.695924800583484 with std: 0.051212978879204014\n",
            "Best hyperparameters combination: 0\n",
            "Test accuracy: 0.7586666666666666 0.046147107770211984\n",
            "Test f1-score: 0.7343396686555153 0.04714610589450773\n",
            "Test auc: 0.695924800583484 0.051212978879204014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_experiments = 3\n",
        "\n",
        "hyperparams_search = [{'d1':816, 'd2':912, 'd3':1024, 'h_dot':2, 'num_heads':2, 'beta':2, 'gamma':1, 'batch_size':1024, 'n_epochs':20, 'verbose':True, 'lr':0.001}]\n",
        "seeds = [i for i in range(n_experiments)]\n",
        "\n",
        "validation_mean_acc = []\n",
        "test_mean_acc = []\n",
        "validation_std_acc = []\n",
        "test_std_acc = []\n",
        "\n",
        "validation_mean_f1 = []\n",
        "test_mean_f1 = []\n",
        "validation_std_f1 = []\n",
        "test_std_f1 = []\n",
        "\n",
        "validation_mean_auc = []\n",
        "test_mean_auc = []\n",
        "validation_std_auc = []\n",
        "test_std_auc = []\n",
        "\n",
        "for p, args in enumerate(hyperparams_search) :\n",
        "\n",
        "  val_seed_acc = []\n",
        "  test_seed_acc = []\n",
        "\n",
        "  val_seed_f1 = []\n",
        "  test_seed_f1 = []\n",
        "\n",
        "  val_seed_auc = []\n",
        "  test_seed_auc = []\n",
        "\n",
        "  for k, s in enumerate(seeds) :\n",
        "    \n",
        "    torch.manual_seed(s)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    train_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/train_dgl_graph_'+str(s)+'.bin')\n",
        "    val_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "    test_dataset = ToxicCommentDataset(save_dir = '/content/drive/My Drive/IASD_tmp/NLP/val_dgl_graph_'+str(s)+'.bin')\n",
        "\n",
        "    train_dataloader = GraphDataLoader(train_dataset,batch_size=args['batch_size'],num_workers=0, drop_last=False)\n",
        "    val_dataloader = GraphDataLoader(val_dataset, batch_size = args['batch_size'],  num_workers=0, drop_last = False)\n",
        "    test_dataloader = GraphDataLoader(test_dataset, batch_size=args['batch_size'],  num_workers=0, drop_last=False)\n",
        "\n",
        "    labels_weights = train_dataset[:][1].numpy()\n",
        "    #class_weights = torch.from_numpy(compute_class_weight('balanced', classes = np.unique(labels_weights), y=labels_weights))\n",
        "    #class_weights = torch.cat((class_weights[0:2], torch.tensor([0.001]), class_weights[2::]))\n",
        "    #class_weights = class_weights.to(dtype = torch.float32).to(device)\n",
        "\n",
        "    model = MLGNN(d0, args['d1'],args['d2'], args['d3'], args['h_dot'], args['h_dot'], args['gamma'], args['num_heads'], c)\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "    epoch_losses = []\n",
        "    for epoch in range(args['n_epochs']):\n",
        "      epoch_loss = 0\n",
        "      iter = 0\n",
        "      for batched_graph, labels in train_dataloader:\n",
        "          batched_graph = batched_graph.to(device)\n",
        "          labels = labels.to(device)\n",
        "          logits = model(batched_graph)\n",
        "          loss = F.cross_entropy(logits, labels) #weight = class_weights)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += loss.detach().item()\n",
        "          iter += 1\n",
        "\n",
        "      epoch_loss /= (iter + 1)\n",
        "      if args['verbose']:\n",
        "        print('Epoch {}, loss {:.4f}'.format(epoch, epoch_loss))\n",
        "      epoch_losses.append(epoch_loss)\n",
        "    \n",
        "    val_seed_acc.append(evaluate(val_dataloader, model))\n",
        "    test_seed_acc.append(evaluate(test_dataloader, model))\n",
        "\n",
        "    val_preds_proba, val_preds_labels, val_labels = predict(val_dataloader, model)\n",
        "    test_preds_proba, test_preds_labels, test_labels = predict(test_dataloader, model)\n",
        "\n",
        "\n",
        "    val_seed_f1.append(f1_score(val_labels.cpu().numpy(), val_preds_labels.cpu().numpy(), average='weighted'))\n",
        "    test_seed_f1.append(f1_score(test_labels.cpu().numpy(), test_preds_labels.cpu().numpy(), average='weighted'))\n",
        "\n",
        "    val_preds_proba = nn.Softmax(dim=1)(val_preds_proba)\n",
        "    test_preds_proba =  nn.Softmax(dim=1)(test_preds_proba)\n",
        "    val_seed_auc.append(roc_auc_score(val_labels.cpu(), val_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "    test_seed_auc.append(roc_auc_score(test_labels.cpu(), test_preds_proba.cpu(), multi_class=\"ovo\", average=\"weighted\"))\n",
        "\n",
        "\n",
        "\n",
        "  #Mesure incertitude en fct seed\n",
        "  validation_mean_acc.append(np.mean(val_seed_acc))  \n",
        "  test_mean_acc.append(np.mean(test_seed_acc))\n",
        "  validation_std_acc.append(np.std(val_seed_acc))  \n",
        "  test_std_acc.append(np.std(test_seed_acc))\n",
        "\n",
        "  validation_mean_f1.append(np.mean(val_seed_f1)) \n",
        "  test_mean_f1.append(np.mean(test_seed_f1))\n",
        "  validation_std_f1.append(np.std(val_seed_f1)) \n",
        "  test_std_f1.append(np.std(test_seed_f1))\n",
        "\n",
        "  validation_mean_auc.append(np.mean(val_seed_auc))\n",
        "  test_mean_auc.append(np.mean(test_seed_auc))\n",
        "  validation_std_auc.append(np.std(val_seed_auc))\n",
        "  test_std_auc.append(np.std(test_seed_auc))\n",
        "  print(\"Metrics for \", p, \"combination:\\n\")\n",
        "  print(\"Validation acc:\", np.mean(val_seed_acc), \"with std:\", np.std(val_seed_acc))\n",
        "  print(\"Validation f1-score:\",np.mean(val_seed_f1), \"with std:\", np.std(val_seed_f1))\n",
        "  print(\"Validation auc:\",np.mean(val_seed_auc), \"with std:\", np.std(val_seed_auc))\n",
        "  print(\"Test acc:\", np.mean(test_seed_acc), \"with std:\", np.std(test_seed_acc))\n",
        "  print(\"Test f1-score:\", np.mean(test_seed_f1), \"with std:\", np.std(test_seed_f1))\n",
        "  print(\"Test auc:\",np.mean(test_seed_auc), \"with std:\", np.std(test_seed_auc))\n",
        "\n",
        "#Records metrics\n",
        "best_val_acc = np.argmax(validation_mean_acc)\n",
        "best_val_f1 = np.argmax(validation_mean_f1)\n",
        "best_val_auc = np.argmax(validation_mean_auc)\n",
        "\n",
        "#Choose the most important criterion (acc, f1 or AUC ?)\n",
        "i = best_val_acc\n",
        "print(\"Best hyperparameters combination:\", i)\n",
        "print(\"Test accuracy:\", test_mean_acc[i], test_std_acc[i])\n",
        "print(\"Test f1-score:\", test_mean_f1[i], test_std_f1[i])\n",
        "print(\"Test auc:\", test_mean_auc[i], test_std_auc[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sRMniMzN1Tr",
        "outputId": "dbf3391e-d5e8-41a1-a1d9-bee13640c412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss 1.2130\n",
            "Epoch 1, loss 0.9125\n",
            "Epoch 2, loss 0.8331\n",
            "Epoch 3, loss 0.8152\n",
            "Epoch 4, loss 0.7722\n",
            "Epoch 5, loss 0.7126\n",
            "Epoch 6, loss 0.6483\n",
            "Epoch 7, loss 0.6354\n",
            "Epoch 8, loss 0.5774\n",
            "Epoch 9, loss 0.5423\n",
            "Epoch 10, loss 0.5485\n",
            "Epoch 11, loss 0.5144\n",
            "Epoch 12, loss 0.5074\n",
            "Epoch 13, loss 0.5349\n",
            "Epoch 14, loss 0.5331\n",
            "Epoch 15, loss 0.5729\n",
            "Epoch 16, loss 0.5713\n",
            "Epoch 17, loss 0.5685\n",
            "Epoch 18, loss 0.5465\n",
            "Epoch 19, loss 0.5112\n",
            "Epoch 0, loss 1.2776\n",
            "Epoch 1, loss 0.9291\n",
            "Epoch 2, loss 0.8449\n",
            "Epoch 3, loss 0.8053\n",
            "Epoch 4, loss 0.7479\n",
            "Epoch 5, loss 0.6771\n",
            "Epoch 6, loss 0.6296\n",
            "Epoch 7, loss 0.6722\n",
            "Epoch 8, loss 0.6579\n",
            "Epoch 9, loss 0.5764\n",
            "Epoch 10, loss 0.5470\n",
            "Epoch 11, loss 0.5606\n",
            "Epoch 12, loss 0.5452\n",
            "Epoch 13, loss 0.5112\n",
            "Epoch 14, loss 0.5869\n",
            "Epoch 15, loss 0.8927\n",
            "Epoch 16, loss 0.7710\n",
            "Epoch 17, loss 0.6823\n",
            "Epoch 18, loss 0.6740\n",
            "Epoch 19, loss 0.5880\n",
            "Epoch 0, loss 1.2259\n",
            "Epoch 1, loss 0.9230\n",
            "Epoch 2, loss 0.8485\n",
            "Epoch 3, loss 0.8084\n",
            "Epoch 4, loss 0.7179\n",
            "Epoch 5, loss 0.6706\n",
            "Epoch 6, loss 0.5921\n",
            "Epoch 7, loss 0.5853\n",
            "Epoch 8, loss 0.5553\n",
            "Epoch 9, loss 0.5303\n",
            "Epoch 10, loss 0.5021\n",
            "Epoch 11, loss 0.5068\n",
            "Epoch 12, loss 0.4939\n",
            "Epoch 13, loss 0.4918\n",
            "Epoch 14, loss 0.4798\n",
            "Epoch 15, loss 0.4785\n",
            "Epoch 16, loss 0.4867\n",
            "Epoch 17, loss 0.4906\n",
            "Epoch 18, loss 0.4935\n",
            "Epoch 19, loss 0.4994\n",
            "Metrics for  0 combination:\n",
            "\n",
            "Validation acc: 0.8006666666666667 with std: 0.00654896090146284\n",
            "Validation f1-score: 0.778720774272997 with std: 0.0050584778370795095\n",
            "Validation auc: 0.7166330759305053 with std: 0.01826603443414899\n",
            "Test acc: 0.8006666666666667 with std: 0.00654896090146284\n",
            "Test f1-score: 0.778720774272997 with std: 0.0050584778370795095\n",
            "Test auc: 0.7166330759305053 with std: 0.01826603443414899\n",
            "Best hyperparameters combination: 0\n",
            "Test accuracy: 0.8006666666666667 0.00654896090146284\n",
            "Test f1-score: 0.778720774272997 0.0050584778370795095\n",
            "Test auc: 0.7166330759305053 0.01826603443414899\n"
          ]
        }
      ]
    }
  ]
}