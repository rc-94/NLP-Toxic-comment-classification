{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vffi7Ee-KUSX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nuQJ-Y2De8sC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "#from transformers import GPT2Tokenizer, GPT2Model\n",
        "#import nltk\n",
        "#from keras_preprocessing.text import Tokenizer\n",
        "from torch import nn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import coo_matrix, vstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vHDOC0T3fclR"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # TF 2.1\n",
        "random.seed(SEED)\n",
        "#seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyNRccgV1f8",
        "outputId": "2fef2d0d-8518-499a-eb62-9da6594047d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_akfTWMaRW"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCg9t1LYc5gq"
      },
      "outputs": [],
      "source": [
        "def sentence_process(sentence):\n",
        "  comment = sentence.replace('\\n', ' ')\n",
        "  while '\\\"' in comment :\n",
        "    comment = comment.replace('\\\"', '')\n",
        "  while \"\\'\" in comment :\n",
        "    comment = comment.replace(\"\\'\", '')\n",
        "  while ':' in comment :\n",
        "    comment = comment.replace(':', '')\n",
        "  while '.' in comment :\n",
        "    comment = comment.replace('.', '')\n",
        "  while '@' in comment :\n",
        "    comment = comment.replace('@', '')\n",
        "  while '+' in comment :\n",
        "    comment = comment.replace('+', '')\n",
        "  while '=' in comment:\n",
        "    comment = comment.replace('=', '')\n",
        "  while '&' in comment :\n",
        "    comment = comment.replace('&', '')\n",
        "  while ')' in comment or '(' in comment:\n",
        "    comment = comment.replace(')', '').replace('(', '')\n",
        "  while ',' in comment or ':' in comment or ';' in comment:\n",
        "    comment = comment.replace(\":\", '').replace(',', '').replace(';', '')\n",
        "  for c in comment :\n",
        "    if c.isascii() == False :\n",
        "      comment = comment.replace(c, '')\n",
        "  n1 = comment.count('!')\n",
        "  n2 = comment.count('?')\n",
        "  for i in range(n1):\n",
        "    comment = comment.replace('!', ' !')\n",
        "  for i in range(n2):\n",
        "    comment = comment.replace('?', ' ?')\n",
        "  while '  ' in comment :\n",
        "    comment = comment.replace('  ', ' ')\n",
        "  return comment\n",
        "\n",
        "def bow(sentence,dic):\n",
        "  v=np.zeros(len(dic))\n",
        "  for word in dic :\n",
        "    words = sentence.split(' ')\n",
        "    for w in words :\n",
        "      if word == w:\n",
        "        v[dic[word]]+=1\n",
        "  return(coo_matrix(v))\n",
        "\n",
        "class BOWDataset :\n",
        "    def __init__(self, filepath, n_words):\n",
        "        self.filepath = filepath\n",
        "        self.n_words = n_words\n",
        "\n",
        "    def process(self):\n",
        "        tic = time.time()\n",
        "\n",
        "        train_df = pd.read_csv(self.filepath + 'train.csv')\n",
        "        tic = time.time()\n",
        "\n",
        "        #delete comments that belongs to several classes\n",
        "        idx_to_del = []\n",
        "        for row in train_df.itertuples():\n",
        "          if sum(row[3::])> 1 :\n",
        "            idx_to_del.append(row[0])\n",
        "          \n",
        "        train_df = train_df.drop(idx_to_del)\n",
        "\n",
        "        cpt = 0\n",
        "        n_too_long = 0\n",
        "        toxic = train_df[train_df['toxic'] == 0]\n",
        "        obscene = toxic[toxic['obscene']==0]\n",
        "        threat = obscene[obscene['threat']==0]\n",
        "        insult = threat[threat['insult']==0]\n",
        "        neutral = insult[insult['identity_hate']==0] #df with neutral comment\n",
        "        toxic = train_df[train_df['toxic'] == 1]\n",
        "        obscene = train_df[train_df['obscene'] == 1]\n",
        "        threat = train_df[train_df['threat'] == 1]\n",
        "        insult = train_df[train_df['insult'] == 1]\n",
        "        identity_hate = train_df[train_df['identity_hate'] == 1]\n",
        "        final_df = pd.concat([toxic, obscene, threat, insult, identity_hate, neutral]) #merge all the df together in the right order\n",
        "        \n",
        "        dic = {}\n",
        "        n_dic=0\n",
        "        if False :\n",
        "          for row in final_df.itertuples():\n",
        "            \n",
        "            cpt = cpt + 1\n",
        "            if cpt in [1344, 1345, 1445, 1446, 1528, 1529, 3243, 3244, 3773, 3776, 11888] :\n",
        "                continue\n",
        "\n",
        "            if n_dic>=10000:\n",
        "                break\n",
        "            print(\"cpt dic:\", cpt)\n",
        "            \n",
        "            comment = sentence_process(row[2])\n",
        "            words = comment.split(' ')\n",
        "            while '' in words :\n",
        "              words.remove('')\n",
        "            for w in words:\n",
        "              if w.isnumeric() and len(w)>=5:\n",
        "                words.remove(w)\n",
        "            words = list(filter(lambda s: 'http' not in s, words))\n",
        "            comment = ' '.join(words)\n",
        "\n",
        "            if len(words)>100 : \n",
        "              print(\"Too long\")\n",
        "              n_too_long += 1\n",
        "              continue\n",
        "\n",
        "            n_dic=n_dic+1\n",
        "            for word in words:\n",
        "              if word not in dic:\n",
        "                dic[word]=len(dic)\n",
        "\n",
        "        #with open(self.filepath +'dic.pickle', 'wb') as handle:\n",
        "          #pickle.dump(dic, handle)\n",
        "\n",
        "        with open(self.filepath +'dic.pickle', 'rb') as handle:\n",
        "          dic = pickle.load(handle)\n",
        "\n",
        "        if True:\n",
        "          cpt = 0\n",
        "          X = []\n",
        "          self.labels = []\n",
        "          for row in final_df.itertuples():\n",
        "        \n",
        "            cpt = cpt + 1\n",
        "            print(\"cpt X\", cpt)\n",
        "\n",
        "            if cpt in [1344, 1345, 1445, 1446, 1528, 1529, 3243, 3244, 3773, 3776, 11888] :\n",
        "              continue\n",
        "\n",
        "            if np.shape(X)[0] == self.n_words and np.shape(self.labels[:self.n_words])[0] == self.n_words:\n",
        "              break\n",
        "            \n",
        "            comment = sentence_process(row[2])\n",
        "            words = comment.split(' ')\n",
        "            while '' in words :\n",
        "              words.remove('')\n",
        "            for w in words:\n",
        "              if w.isnumeric() and len(w)>=5:\n",
        "                words.remove(w)\n",
        "            words = list(filter(lambda s: 'http' not in s, words))\n",
        "            comment = ' '.join(words)\n",
        "\n",
        "            if len(words)>100 : \n",
        "              print(\"Too long\")\n",
        "              n_too_long += 1\n",
        "              continue\n",
        "\n",
        "            if row[3] == 0 and row[4] == 0 and row[5] ==0 and row[6] ==0 and row[7] == 0 and row[8] == 0:\n",
        "              self.labels.append(0)\n",
        "            elif row[3] == 1 :\n",
        "              self.labels.append(1)\n",
        "            elif row[4] == 1 :\n",
        "              self.labels.append(6)\n",
        "            elif row[5] == 1 :\n",
        "              self.labels.append(2)\n",
        "            elif row[6] == 1 :\n",
        "              self.labels.append(3)\n",
        "            elif row[7] == 1 :\n",
        "              self.labels.append(4)\n",
        "            else: \n",
        "              self.labels.append(5)\n",
        "\n",
        "            X.append(bow(comment,dic))\n",
        "\n",
        "          X = vstack(X)\n",
        "          labels = np.array(self.labels)\n",
        "          print(\"labels\", np.unique(labels))\n",
        "        with open('/content/drive/My Drive/NLP/X_10000.pickle', 'wb') as handle:\n",
        "          pickle.dump(X, handle)\n",
        "\n",
        "        with open('/content/drive/My Drive/NLP/labels_10000.pickle', 'wb') as handle: \n",
        "          pickle.dump(labels, handle)\n",
        "        #with open('/content/drive/My Drive/NLP/X_10000.pickle', 'rb') as handle:\n",
        "          #X=pickle.load(handle)\n",
        "\n",
        "        #with open('/content/drive/My Drive/NLP/labels_10000.pickle', 'rb') as handle: \n",
        "          #labels=pickle.load(handle)\n",
        "        return X, labels\n",
        "\n",
        "X, labels = BOWDataset(filepath = '/content/drive/My Drive/NLP/', n_words = 10000).process()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Tvg8Gh62Uv",
        "outputId": "26d908c2-a09d-4b9c-cd4e-4620c79c817a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random state :  0 \n",
            "\n",
            "Random state :  1 \n",
            "\n",
            "Random state :  2 \n",
            "\n",
            "Accuracy :  0.7566666666666667 with std 0.004784233364802446\n",
            "f1_score :  0.7531432264466863 with std 0.006082794528862612\n",
            "AUC :  0.7406805422311146 with std 0.034569295289029724\n",
            "time :  828.293196439743\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "random_states = [0,1,2]\n",
        "\n",
        "accuracy = []\n",
        "F1_Score=[]\n",
        "AUC=[]\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "#class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(labels),y=labels)\n",
        "#print(class_weights)\n",
        "\n",
        "for r in random_states :\n",
        "\n",
        "  print(\"Random state : \", r, \"\\n\")\n",
        "  X, labels = shuffle(X, labels, random_state=r)\n",
        "  num_examples = np.shape(X)[0]\n",
        "  num_train = int(0.80 * num_examples)\n",
        "  num_val = int((num_examples-num_train)/2)\n",
        "  X_train, y_train = X[:num_train], labels[:num_train]\n",
        "  X_test, y_test = X[(num_train+num_val) : (num_train+2*num_val)], labels[(num_train+num_val):(num_train+2*num_val)]\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.80,random_state=r)\n",
        "  pipeline = Pipeline([('clf', LogisticRegression(random_state = r,class_weight=\"balanced\"))])\n",
        "\n",
        "  params = {'clf__C': [1.0,0.5,0.1], 'clf__max_iter':[500,1000]}\n",
        "\n",
        "\n",
        "  rskf = StratifiedKFold(n_splits=6, random_state=r, shuffle=True)\n",
        "\n",
        "  cv = GridSearchCV(pipeline, params, cv = rskf, scoring = 'accuracy')\n",
        "  cv.fit(X_train, y_train)\n",
        "\n",
        "  #print(f'Best accuracy -score: {cv.best_score_:.3f}\\n')\n",
        "  #print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "  #print(f'Scores: {classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "  preds_proba = cv.predict_proba(X_test)\n",
        "  #print(preds_proba)\n",
        "  preds = cv.predict(X_test)\n",
        "  #print(f'Scores: {classification_report(y_test, preds)}\\n')\n",
        "  #print(f'accuracy score: {accuracy_score(y_test, preds):.3f}')\n",
        "  accuracy.append(accuracy_score(y_test, preds))\n",
        "  F1_Score.append(f1_score(y_test, preds, average='weighted'))\n",
        "  AUC.append(roc_auc_score(y_test,preds_proba,multi_class=\"ovo\",average=\"weighted\"))\n",
        "\n",
        "tac = time.time()\n",
        "\n",
        "print(\"Accuracy : \",np.mean(accuracy),\"with std\",np.std(accuracy))\n",
        "print(\"f1_score : \",np.mean(F1_Score),\"with std\",np.std(F1_Score))\n",
        "print(\"AUC : \",np.mean(AUC),\"with std\",np.std(AUC))\n",
        "print(\"time : \", tac-tic)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CamCam_Project_NLP2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}