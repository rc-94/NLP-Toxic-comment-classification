{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vffi7Ee-KUSX",
        "outputId": "abd496b0-dfdf-42d5-9dad-bbc0a311437d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 10.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nuQJ-Y2De8sC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "import nltk\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from torch import nn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.sparse import coo_matrix, vstack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgQEk9Y7Dy5G",
        "outputId": "0ed894a7-4c18-457f-fc4e-48beb05b8396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vHDOC0T3fclR"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # TF 2.1\n",
        "random.seed(SEED)\n",
        "#seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyNRccgV1f8",
        "outputId": "2fef2d0d-8518-499a-eb62-9da6594047d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs_akfTWMaRW"
      },
      "source": [
        "# BOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCg9t1LYc5gq"
      },
      "outputs": [],
      "source": [
        "def sentence_process(sentence):\n",
        "  comment = sentence.replace('\\n', ' ')\n",
        "  while '\\\"' in comment :\n",
        "    comment = comment.replace('\\\"', '')\n",
        "  while \"\\'\" in comment :\n",
        "    comment = comment.replace(\"\\'\", '')\n",
        "  while ':' in comment :\n",
        "    comment = comment.replace(':', '')\n",
        "  while '.' in comment :\n",
        "    comment = comment.replace('.', '')\n",
        "  while '@' in comment :\n",
        "    comment = comment.replace('@', '')\n",
        "  while '+' in comment :\n",
        "    comment = comment.replace('+', '')\n",
        "  while '=' in comment:\n",
        "    comment = comment.replace('=', '')\n",
        "  while '&' in comment :\n",
        "    comment = comment.replace('&', '')\n",
        "  while ')' in comment or '(' in comment:\n",
        "    comment = comment.replace(')', '').replace('(', '')\n",
        "  while ',' in comment or ':' in comment or ';' in comment:\n",
        "    comment = comment.replace(\":\", '').replace(',', '').replace(';', '')\n",
        "  for c in comment :\n",
        "    if c.isascii() == False :\n",
        "      comment = comment.replace(c, '')\n",
        "  n1 = comment.count('!')\n",
        "  n2 = comment.count('?')\n",
        "  for i in range(n1):\n",
        "    comment = comment.replace('!', ' !')\n",
        "  for i in range(n2):\n",
        "    comment = comment.replace('?', ' ?')\n",
        "  while '  ' in comment :\n",
        "    comment = comment.replace('  ', ' ')\n",
        "  return comment\n",
        "\n",
        "def bow(sentence,dic):\n",
        "  v=np.zeros(len(dic))\n",
        "  for word in dic :\n",
        "    words = sentence.split(' ')\n",
        "    for w in words :\n",
        "      if word == w:\n",
        "        v[dic[word]]+=1\n",
        "  return(coo_matrix(v))\n",
        "\n",
        "class BOWDataset :\n",
        "    def __init__(self, filepath, n_words):\n",
        "        self.filepath = filepath\n",
        "        self.n_words = n_words\n",
        "\n",
        "    def process(self):\n",
        "        tic = time.time()\n",
        "\n",
        "        train_df = pd.read_csv(self.filepath + 'train.csv')\n",
        "        tic = time.time()\n",
        "\n",
        "        #delete comments that belongs to several classes\n",
        "        idx_to_del = []\n",
        "        for row in train_df.itertuples():\n",
        "          if sum(row[3::])> 1 :\n",
        "            idx_to_del.append(row[0])\n",
        "          \n",
        "        train_df = train_df.drop(idx_to_del)\n",
        "\n",
        "        cpt = 0\n",
        "        n_too_long = 0\n",
        "        toxic = train_df[train_df['toxic'] == 0]\n",
        "        obscene = toxic[toxic['obscene']==0]\n",
        "        threat = obscene[obscene['threat']==0]\n",
        "        insult = threat[threat['insult']==0]\n",
        "        neutral = insult[insult['identity_hate']==0] #df with neutral comment\n",
        "        toxic = train_df[train_df['toxic'] == 1]\n",
        "        obscene = train_df[train_df['obscene'] == 1]\n",
        "        threat = train_df[train_df['threat'] == 1]\n",
        "        insult = train_df[train_df['insult'] == 1]\n",
        "        identity_hate = train_df[train_df['identity_hate'] == 1]\n",
        "        final_df = pd.concat([toxic, obscene, threat, insult, identity_hate, neutral]) #merge all the df together in the right order\n",
        "        \n",
        "        dic = {}\n",
        "        n_dic=0\n",
        "        if False :\n",
        "          for row in final_df.itertuples():\n",
        "            \n",
        "            cpt = cpt + 1\n",
        "            if cpt in [1344, 1345, 1445, 1446, 1528, 1529, 3243, 3244, 3773, 3776, 11888] :\n",
        "                continue\n",
        "\n",
        "            if n_dic>=10000:\n",
        "                break\n",
        "            print(\"cpt dic:\", cpt)\n",
        "            \n",
        "            comment = sentence_process(row[2])\n",
        "            words = comment.split(' ')\n",
        "            while '' in words :\n",
        "              words.remove('')\n",
        "            for w in words:\n",
        "              if w.isnumeric() and len(w)>=5:\n",
        "                words.remove(w)\n",
        "            words = list(filter(lambda s: 'http' not in s, words))\n",
        "            comment = ' '.join(words)\n",
        "\n",
        "            if len(words)>100 : \n",
        "              print(\"Too long\")\n",
        "              n_too_long += 1\n",
        "              continue\n",
        "\n",
        "            n_dic=n_dic+1\n",
        "            for word in words:\n",
        "              if word not in dic:\n",
        "                dic[word]=len(dic)\n",
        "\n",
        "        #with open(self.filepath +'dic.pickle', 'wb') as handle:\n",
        "          #pickle.dump(dic, handle)\n",
        "\n",
        "        with open(self.filepath +'dic.pickle', 'rb') as handle:\n",
        "          dic = pickle.load(handle)\n",
        "\n",
        "        if True:\n",
        "          cpt = 0\n",
        "          X = []\n",
        "          self.labels = []\n",
        "          for row in final_df.itertuples():\n",
        "        \n",
        "            cpt = cpt + 1\n",
        "            print(\"cpt X\", cpt)\n",
        "\n",
        "            if cpt in [1344, 1345, 1445, 1446, 1528, 1529, 3243, 3244, 3773, 3776, 11888] :\n",
        "              continue\n",
        "\n",
        "            if np.shape(X)[0] == self.n_words and np.shape(self.labels[:self.n_words])[0] == self.n_words:\n",
        "              break\n",
        "            \n",
        "            comment = sentence_process(row[2])\n",
        "            words = comment.split(' ')\n",
        "            while '' in words :\n",
        "              words.remove('')\n",
        "            for w in words:\n",
        "              if w.isnumeric() and len(w)>=5:\n",
        "                words.remove(w)\n",
        "            words = list(filter(lambda s: 'http' not in s, words))\n",
        "            comment = ' '.join(words)\n",
        "\n",
        "            if len(words)>100 : \n",
        "              print(\"Too long\")\n",
        "              n_too_long += 1\n",
        "              continue\n",
        "\n",
        "            if row[3] == 0 and row[4] == 0 and row[5] ==0 and row[6] ==0 and row[7] == 0 and row[8] == 0:\n",
        "              self.labels.append(0)\n",
        "            elif row[3] == 1 :\n",
        "              self.labels.append(1)\n",
        "            elif row[4] == 1 :\n",
        "              self.labels.append(6)\n",
        "            elif row[5] == 1 :\n",
        "              self.labels.append(2)\n",
        "            elif row[6] == 1 :\n",
        "              self.labels.append(3)\n",
        "            elif row[7] == 1 :\n",
        "              self.labels.append(4)\n",
        "            else: \n",
        "              self.labels.append(5)\n",
        "\n",
        "            X.append(bow(comment,dic))\n",
        "\n",
        "          X = vstack(X)\n",
        "          labels = np.array(self.labels)\n",
        "          print(\"labels\", np.unique(labels))\n",
        "        with open('/content/drive/My Drive/NLP/X_10000.pickle', 'wb') as handle:\n",
        "          pickle.dump(X, handle)\n",
        "\n",
        "        with open('/content/drive/My Drive/NLP/labels_10000.pickle', 'wb') as handle: \n",
        "          pickle.dump(labels, handle)\n",
        "        #with open('/content/drive/My Drive/NLP/X_10000.pickle', 'rb') as handle:\n",
        "          #X=pickle.load(handle)\n",
        "\n",
        "        #with open('/content/drive/My Drive/NLP/labels_10000.pickle', 'rb') as handle: \n",
        "          #labels=pickle.load(handle)\n",
        "        return X, labels\n",
        "\n",
        "X, labels = BOWDataset(filepath = '/content/drive/My Drive/NLP/', n_words = 10000).process()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Tvg8Gh62Uv",
        "outputId": "26d908c2-a09d-4b9c-cd4e-4620c79c817a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random state :  0 \n",
            "\n",
            "Random state :  1 \n",
            "\n",
            "Random state :  2 \n",
            "\n",
            "Accuracy :  0.7566666666666667 with std 0.004784233364802446\n",
            "f1_score :  0.7531432264466863 with std 0.006082794528862612\n",
            "AUC :  0.7406805422311146 with std 0.034569295289029724\n",
            "time :  828.293196439743\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "random_states = [0,1,2]\n",
        "\n",
        "accuracy = []\n",
        "F1_Score=[]\n",
        "AUC=[]\n",
        "\n",
        "tic = time.time()\n",
        "\n",
        "#class_weights = compute_class_weight(class_weight='balanced',classes=np.unique(labels),y=labels)\n",
        "#print(class_weights)\n",
        "\n",
        "for r in random_states :\n",
        "\n",
        "  print(\"Random state : \", r, \"\\n\")\n",
        "  X, labels = shuffle(X, labels, random_state=r)\n",
        "  num_examples = np.shape(X)[0]\n",
        "  num_train = int(0.80 * num_examples)\n",
        "  num_val = int((num_examples-num_train)/2)\n",
        "  X_train, y_train = X[:num_train], labels[:num_train]\n",
        "  X_test, y_test = X[(num_train+num_val) : (num_train+2*num_val)], labels[(num_train+num_val):(num_train+2*num_val)]\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.80,random_state=r)\n",
        "  pipeline = Pipeline([('clf', LogisticRegression(random_state = r,class_weight=\"balanced\"))])\n",
        "\n",
        "  params = {'clf__C': [1.0,0.5,0.1], 'clf__max_iter':[500,1000]}\n",
        "\n",
        "\n",
        "  rskf = StratifiedKFold(n_splits=6, random_state=r, shuffle=True)\n",
        "\n",
        "  cv = GridSearchCV(pipeline, params, cv = rskf, scoring = 'accuracy')\n",
        "  cv.fit(X_train, y_train)\n",
        "\n",
        "  #print(f'Best accuracy -score: {cv.best_score_:.3f}\\n')\n",
        "  #print(f'Best parameter set: {cv.best_params_}\\n')\n",
        "  #print(f'Scores: {classification_report(y_train, cv.predict(X_train))}')\n",
        "\n",
        "  preds_proba = cv.predict_proba(X_test)\n",
        "  #print(preds_proba)\n",
        "  preds = cv.predict(X_test)\n",
        "  #print(f'Scores: {classification_report(y_test, preds)}\\n')\n",
        "  #print(f'accuracy score: {accuracy_score(y_test, preds):.3f}')\n",
        "  accuracy.append(accuracy_score(y_test, preds))\n",
        "  F1_Score.append(f1_score(y_test, preds, average='weighted'))\n",
        "  AUC.append(roc_auc_score(y_test,preds_proba,multi_class=\"ovo\",average=\"weighted\"))\n",
        "\n",
        "tac = time.time()\n",
        "\n",
        "print(\"Accuracy : \",np.mean(accuracy),\"with std\",np.std(accuracy))\n",
        "print(\"f1_score : \",np.mean(F1_Score),\"with std\",np.std(F1_Score))\n",
        "print(\"AUC : \",np.mean(AUC),\"with std\",np.std(AUC))\n",
        "print(\"time : \", tac-tic)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CamCam_Project_NLP2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}